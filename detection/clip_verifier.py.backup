"""Secondary verification stage (clip-based) with 3D CNN support.

The second stage of the ThreatDetect pipeline analyses candidate events
produced by :class:`detection.frame_detector.FrameDetector`. It inspects a short
video clip around the trigger frame (typically 10 seconds before and
10 seconds after) with a 3D CNN action recognition model to determine whether the
event truly represents a weapon threat, violent behavior, or other suspicious actions.

This module supports multiple 3D CNN backends:
- TAO ActionRecognitionNet (default): NVIDIA's optimized models with native DeepStream integration
- X3D: Facebook's efficient 3D CNNs (X3D-S, X3D-M) optimized for mobile/edge deployment
- Simple aggregation: Fallback method that counts weapon detections across frames

Notes:
- Model loading for TensorRT engines is stubbed for portability in this
  repository; production deployments should implement the full TensorRT
  engine I/O (bindings and buffer management) where indicated.
- The `simple` verifier is intentionally lightweight and used as a safe
  fallback when heavy dependencies (PyTorch/TensorRT) are unavailable.
"""
    fallback when heavy dependencies (PyTorch/TensorRT) are unavailable.
"""

from __future__ import annotations

import logging
import os
from typing import Dict, Iterable, List, Optional, Union
import numpy as np

try:
    import cv2
except ImportError:
    cv2 = None

try:
    import torch
    import torchvision.transforms as transforms
except ImportError:
    torch = None
    transforms = None

try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
except ImportError:
    trt = None
    cuda = None

LOGGER = logging.getLogger(__name__)


class ClipVerifier:
    """Secondary clip-based verification with 3D CNN support.

    Parameters
    ----------
    model_path : str
        Path to the 3D CNN model (TensorRT engine, ONNX, or PyTorch checkpoint).
    model_type : str
        Type of 3D CNN model. Options: 'tao' (default), 'x3d', 'simple'.
    threshold : float
        Score threshold above which the event is considered verified.
    input_size : tuple
        Spatial dimensions for video input (height, width). Default (224, 224).
    temporal_size : int
        Number of frames to sample from the clip. Default 16.
    """

    def __init__(
        self, 
        model_path: str, 
        model_type: str = "tao",
        threshold: float = 0.7,
        input_size: tuple = (224, 224),
        temporal_size: int = 16
    ) -> None:
        self.model_path = model_path
        self.model_type = model_type.lower()
        self.threshold = threshold
        self.input_size = input_size
        self.temporal_size = temporal_size
        self.model = None
        self.action_classes = self._get_action_classes()
        
        if model_path and os.path.exists(model_path):
            self._load_model()
            LOGGER.info("ClipVerifier initialized with %s model: %s", model_type, model_path)
        else:
            LOGGER.warning("Model path not found, falling back to simple aggregation: %s", model_path)
            self.model_type = "simple"

    def _get_action_classes(self) -> Dict[str, int]:
        """Define action classes for different model types."""
        if self.model_type == "tao":
            # TAO ActionRecognitionNet classes (example - adjust based on your training)
            return {
                "normal": 0,
                "threatening_with_weapon": 1, 
                "pointing_weapon": 2,
                "firing_weapon": 3,
                "assault": 4,
                "fighting": 5
            }
        elif self.model_type == "x3d":
            # X3D trained on custom action dataset
            return {
                "normal": 0,
                "weapon_threat": 1,
                "violent_action": 2,
                "suspicious_behavior": 3
            }
        elif self.model_type == "movinet":
            # MoViNet trained for efficient mobile action recognition
            return {
                "normal": 0,
                "weapon_handling": 1,
                "aggressive_behavior": 2,
                "violent_action": 3,
                "suspicious_activity": 4
            }
        else:
            return {}

    def _load_model(self) -> None:
        """Load the 3D CNN model based on model_type."""
        try:
            if self.model_type == "tao":
                self._load_tao_model()
            elif self.model_type == "x3d":
                self._load_x3d_model()
            elif self.model_type == "movinet":
                self._load_movinet_model()
            else:
                LOGGER.warning("Unknown model type: %s, using simple aggregation", self.model_type)
                self.model_type = "simple"
        except Exception as exc:
            LOGGER.error("Failed to load %s model: %s", self.model_type, exc)
            self.model_type = "simple"

    def _load_tao_model(self) -> None:
        """Load TAO ActionRecognitionNet model (TensorRT engine preferred)."""
        if self.model_path.endswith('.engine') and trt is not None:
            # Load TensorRT engine for TAO model
            LOGGER.info("Loading TAO ActionRecognitionNet TensorRT engine")
            with open(self.model_path, 'rb') as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(trt.Logger.WARNING)
            engine = runtime.deserialize_cuda_engine(engine_data)
            
            if engine is None:
                raise RuntimeError("Failed to deserialize TAO TensorRT engine")
                
            context = engine.create_execution_context()
            self.model = {'engine': engine, 'context': context}
            LOGGER.info("TAO TensorRT engine loaded successfully")
            
        else:
            # Fallback to ONNX or other formats
            LOGGER.warning("TAO model loading not implemented for format: %s", self.model_path)
            raise NotImplementedError("TAO ONNX inference not implemented yet")

    def _load_x3d_model(self) -> None:
        """Load X3D model (PyTorch or TensorRT)."""
        if torch is None:
            raise ImportError("PyTorch not available for X3D model loading")
            
        if self.model_path.endswith('.engine') and trt is not None:
            # Load X3D TensorRT engine
            LOGGER.info("Loading X3D TensorRT engine")
            with open(self.model_path, 'rb') as f:
                engine_data = f.read()
                
            runtime = trt.Runtime(trt.Logger.WARNING)
            engine = runtime.deserialize_cuda_engine(engine_data)
            
            if engine is None:
                raise RuntimeError("Failed to deserialize X3D TensorRT engine")
                
            context = engine.create_execution_context()
            self.model = {'engine': engine, 'context': context, 'type': 'tensorrt'}
            LOGGER.info("X3D TensorRT engine loaded successfully")
            
        elif self.model_path.endswith(('.pt', '.pth')):
            # Load PyTorch X3D checkpoint
            LOGGER.info("Loading X3D PyTorch model")
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model = torch.load(self.model_path, map_location=device)
            self.model.eval()
            LOGGER.info("X3D PyTorch model loaded successfully")
            
        else:
            raise ValueError(f"Unsupported X3D model format: {self.model_path}")

    def _load_movinet_model(self) -> None:
        """Load MoViNet model (PyTorch or TensorRT)."""
        if torch is None:
            raise ImportError("PyTorch not available for MoViNet model loading")
            
        if self.model_path.endswith('.engine') and trt is not None:
            # Load MoViNet TensorRT engine
            LOGGER.info("Loading MoViNet TensorRT engine")
            with open(self.model_path, 'rb') as f:
                engine_data = f.read()
                
            runtime = trt.Runtime(trt.Logger.WARNING)
            engine = runtime.deserialize_cuda_engine(engine_data)
            
            if engine is None:
                raise RuntimeError("Failed to deserialize MoViNet TensorRT engine")
                
            context = engine.create_execution_context()
            self.model = {'engine': engine, 'context': context, 'type': 'tensorrt'}
            LOGGER.info("MoViNet TensorRT engine loaded successfully")
            
        elif self.model_path.endswith(('.pt', '.pth')):
            # Load PyTorch MoViNet checkpoint
            LOGGER.info("Loading MoViNet PyTorch model")
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model = torch.load(self.model_path, map_location=device)
            self.model.eval()
            LOGGER.info("MoViNet PyTorch model loaded successfully")
            
        else:
            raise ValueError(f"Unsupported MoViNet model format: {self.model_path}")

    def verify(self, video_clip: Union[np.ndarray, List[np.ndarray]], detections_per_frame: Optional[Iterable[List[str]]] = None) -> Dict[str, float]:
        """Verify a candidate event using 3D CNN analysis.

        Parameters
        ----------
        video_clip : Union[np.ndarray, List[np.ndarray]]
            Video clip as either:
            - np.ndarray of shape (T, H, W, C) where T=temporal_size
            - List of np.ndarray frames, each of shape (H, W, C)
        detections_per_frame : Optional[Iterable[List[str]]]
            For simple aggregation fallback: list of detection labels per frame.
            Used only when 3D CNN models are not available.

        Returns
        -------
        Dict[str, float]
            Dictionary containing:
            - "score": Overall threat confidence (0-1)
            - "action": Predicted action class name
            - "action_confidence": Confidence for the predicted action
            - Individual action class scores (e.g., "weapon_threat", "assault")
        """
        if self.model_type == "simple":
            return self._verify_simple(detections_per_frame)
        elif self.model_type == "tao":
            return self._verify_tao(video_clip)
        elif self.model_type == "x3d":
            return self._verify_x3d(video_clip)
        elif self.model_type == "movinet":
            return self._verify_movinet(video_clip)
        else:
            LOGGER.error("Unknown model type: %s", self.model_type)
            return {"score": 0.0, "action": "unknown", "action_confidence": 0.0}

    def _verify_simple(self, detections_per_frame: Iterable[List[str]]) -> Dict[str, float]:
        """Simple aggregation fallback: count weapon detections across frames."""
        if detections_per_frame is None:
            return {"score": 0.0, "action": "no_data", "action_confidence": 0.0}
            
        total = 0
        weapon_frames = 0
        for labels in detections_per_frame:
            total += 1
            if any(label in {"gun", "knife", "weapon"} for label in labels):
                weapon_frames += 1
                
        score = float(weapon_frames) / float(total) if total else 0.0
        action = "weapon_detected" if score > self.threshold else "normal"
        
        return {
            "score": score,
            "action": action,
            "action_confidence": score,
            "weapon_ratio": score
        }

    def _verify_tao(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> Dict[str, float]:
        """Verify using TAO ActionRecognitionNet model."""
        try:
            # Preprocess video clip for TAO model
            processed_clip = self._preprocess_clip_tao(video_clip)
            
            if self.model and 'engine' in self.model:
                # Run TensorRT inference
                results = self._run_tao_inference(processed_clip)
            else:
                # Fallback for ONNX or other formats
                LOGGER.warning("TAO ONNX inference not implemented, using simple fallback")
                return self._verify_simple(None)
                
            return self._parse_tao_results(results)
            
        except Exception as exc:
            LOGGER.error("TAO inference failed: %s", exc)
            return {"score": 0.0, "action": "error", "action_confidence": 0.0}

    def _verify_x3d(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> Dict[str, float]:
        """Verify using X3D model."""
        try:
            # Preprocess video clip for X3D model
            processed_clip = self._preprocess_clip_x3d(video_clip)
            
            if self.model and isinstance(self.model, dict) and 'engine' in self.model:
                # Run TensorRT inference
                results = self._run_x3d_tensorrt_inference(processed_clip)
            elif torch is not None and hasattr(self.model, 'eval'):
                # Run PyTorch inference
                results = self._run_x3d_pytorch_inference(processed_clip)
            else:
                LOGGER.warning("X3D model not properly loaded, using simple fallback")
                return self._verify_simple(None)
                
            return self._parse_x3d_results(results)
            
        except Exception as exc:
            LOGGER.error("X3D inference failed: %s", exc)
            return {"score": 0.0, "action": "error", "action_confidence": 0.0}

    def _verify_movinet(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> Dict[str, float]:
        """Verify using MoViNet model."""
        try:
            # Preprocess video clip for MoViNet model
            processed_clip = self._preprocess_clip_movinet(video_clip)
            
            if self.model and isinstance(self.model, dict) and 'engine' in self.model:
                # Run TensorRT inference
                results = self._run_movinet_tensorrt_inference(processed_clip)
            elif torch is not None and hasattr(self.model, 'eval'):
                # Run PyTorch inference
                results = self._run_movinet_pytorch_inference(processed_clip)
            else:
                LOGGER.warning("MoViNet model not properly loaded, using simple fallback")
                return self._verify_simple(None)
                
            return self._parse_movinet_results(results)
            
        except Exception as exc:
            LOGGER.error("MoViNet inference failed: %s", exc)
            return {"score": 0.0, "action": "error", "action_confidence": 0.0}

    def _preprocess_clip_tao(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> np.ndarray:
        """Preprocess video clip for TAO ActionRecognitionNet."""
        if cv2 is None:
            raise ImportError("OpenCV not available for video preprocessing")
            
        # Convert list of frames to numpy array if needed
        if isinstance(video_clip, list):
            frames = video_clip
        else:
            frames = [video_clip[i] for i in range(video_clip.shape[0])]
            
        # Sample frames to match temporal_size
        sampled_frames = self._sample_frames(frames, self.temporal_size)
        
        # Resize and normalize for TAO model
        processed_frames = []
        for frame in sampled_frames:
            # Resize to input_size
            resized = cv2.resize(frame, self.input_size)
            # Convert BGR to RGB if needed
            if len(resized.shape) == 3 and resized.shape[2] == 3:
                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            # Normalize to [0, 1]
            normalized = resized.astype(np.float32) / 255.0
            processed_frames.append(normalized)
            
        # Stack frames: (T, H, W, C)
        clip_array = np.stack(processed_frames, axis=0)
        
        # TAO expects (C, T, H, W) format
        clip_array = np.transpose(clip_array, (3, 0, 1, 2))
        
        return clip_array

    def _preprocess_clip_x3d(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> np.ndarray:
        """Preprocess video clip for X3D model."""
        if cv2 is None:
            raise ImportError("OpenCV not available for video preprocessing")
            
        # Convert list of frames to numpy array if needed
        if isinstance(video_clip, list):
            frames = video_clip
        else:
            frames = [video_clip[i] for i in range(video_clip.shape[0])]
            
        # Sample frames to match temporal_size
        sampled_frames = self._sample_frames(frames, self.temporal_size)
        
        # Resize and normalize for X3D model
        processed_frames = []
        for frame in sampled_frames:
            # Resize to input_size  
            resized = cv2.resize(frame, self.input_size)
            # Convert BGR to RGB if needed
            if len(resized.shape) == 3 and resized.shape[2] == 3:
                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            # Normalize using ImageNet stats
            normalized = resized.astype(np.float32) / 255.0
            normalized = (normalized - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
            processed_frames.append(normalized)
            
        # Stack frames: (T, H, W, C)
        clip_array = np.stack(processed_frames, axis=0)
        
        # X3D expects (C, T, H, W) format
        clip_array = np.transpose(clip_array, (3, 0, 1, 2))
        
        return clip_array

    def _preprocess_clip_movinet(self, video_clip: Union[np.ndarray, List[np.ndarray]]) -> np.ndarray:
        """Preprocess video clip for MoViNet model."""
        if cv2 is None:
            raise ImportError("OpenCV not available for video preprocessing")
            
        # Convert list of frames to numpy array if needed
        if isinstance(video_clip, list):
            frames = video_clip
        else:
            frames = [video_clip[i] for i in range(video_clip.shape[0])]
            
        # MoViNet can handle variable frame counts, but we'll sample to temporal_size
        sampled_frames = self._sample_frames(frames, self.temporal_size)
        
        # Resize and normalize for MoViNet model
        processed_frames = []
        for frame in sampled_frames:
            # Resize to input_size
            resized = cv2.resize(frame, self.input_size)
            # Convert BGR to RGB if needed
            if len(resized.shape) == 3 and resized.shape[2] == 3:
                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            # Normalize to [-1, 1] for MoViNet
            normalized = (resized.astype(np.float32) / 127.5) - 1.0
            processed_frames.append(normalized)
            
        # Stack frames: (T, H, W, C)
        clip_array = np.stack(processed_frames, axis=0)
        
        # MoViNet expects (C, T, H, W) format
        clip_array = np.transpose(clip_array, (3, 0, 1, 2))
        
        return clip_array

    def _sample_frames(self, frames: List[np.ndarray], target_count: int) -> List[np.ndarray]:
        """Sample frames uniformly to match target temporal size."""
        if len(frames) <= target_count:
            # Pad with last frame if not enough frames
            sampled = frames + [frames[-1]] * (target_count - len(frames))
        else:
            # Uniformly sample frames
            indices = np.linspace(0, len(frames) - 1, target_count, dtype=int)
            sampled = [frames[i] for i in indices]
        return sampled

    def _run_tao_inference(self, processed_clip: np.ndarray) -> np.ndarray:
        """Run inference using TAO TensorRT engine."""
        # This is a placeholder - actual TensorRT inference implementation
        # would require setting up input/output buffers and running the engine
        LOGGER.warning("TAO TensorRT inference not fully implemented")
        
        # Mock results for demonstration
        num_classes = len(self.action_classes)
        mock_results = np.random.rand(num_classes).astype(np.float32)
        mock_results = mock_results / np.sum(mock_results)  # Normalize to probabilities
        return mock_results

    def _run_x3d_tensorrt_inference(self, processed_clip: np.ndarray) -> np.ndarray:
        """Run inference using X3D TensorRT engine."""
        # This is a placeholder - actual TensorRT inference implementation
        # would require setting up input/output buffers and running the engine
        LOGGER.warning("X3D TensorRT inference not fully implemented")
        
        # Mock results for demonstration
        num_classes = len(self.action_classes)
        mock_results = np.random.rand(num_classes).astype(np.float32)
        mock_results = mock_results / np.sum(mock_results)  # Normalize to probabilities
        return mock_results

    def _run_x3d_pytorch_inference(self, processed_clip: np.ndarray) -> np.ndarray:
        """Run inference using X3D PyTorch model."""
        if torch is None:
            raise ImportError("PyTorch not available")
            
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Convert to tensor and add batch dimension
        input_tensor = torch.from_numpy(processed_clip).unsqueeze(0).to(device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            
        # Apply softmax to get probabilities
        probabilities = torch.softmax(outputs, dim=1)
        
        return probabilities.cpu().numpy().squeeze()

    def _run_movinet_tensorrt_inference(self, processed_clip: np.ndarray) -> np.ndarray:
        """Run inference using MoViNet TensorRT engine."""
        # This is a placeholder - actual TensorRT inference implementation
        # would require setting up input/output buffers and running the engine
        LOGGER.warning("MoViNet TensorRT inference not fully implemented")
        
        # Mock results for demonstration
        num_classes = len(self.action_classes)
        mock_results = np.random.rand(num_classes).astype(np.float32)
        mock_results = mock_results / np.sum(mock_results)  # Normalize to probabilities
        return mock_results

    def _run_movinet_pytorch_inference(self, processed_clip: np.ndarray) -> np.ndarray:
        """Run inference using MoViNet PyTorch model."""
        if torch is None:
            raise ImportError("PyTorch not available")
            
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Convert to tensor and add batch dimension
        input_tensor = torch.from_numpy(processed_clip).unsqueeze(0).to(device)
        
        with torch.no_grad():
            outputs = self.model(input_tensor)
            
        # Apply softmax to get probabilities
        probabilities = torch.softmax(outputs, dim=1)
        
        return probabilities.cpu().numpy().squeeze()

    def _parse_tao_results(self, results: np.ndarray) -> Dict[str, float]:
        """Parse TAO model inference results."""
        class_names = list(self.action_classes.keys())
        
        # Get predicted class and confidence
        predicted_idx = np.argmax(results)
        predicted_action = class_names[predicted_idx] if predicted_idx < len(class_names) else "unknown"
        action_confidence = float(results[predicted_idx])
        
        # Calculate overall threat score
        threat_indices = [1, 2, 3, 4, 5]  # threatening_with_weapon, pointing_weapon, firing_weapon, assault, fighting
        threat_scores = [results[i] for i in threat_indices if i < len(results)]
        overall_score = sum(threat_scores) if threat_scores else 0.0
        
        result = {
            "score": overall_score,
            "action": predicted_action,
            "action_confidence": action_confidence
        }
        
        # Add individual class scores
        for i, class_name in enumerate(class_names):
            if i < len(results):
                result[class_name] = float(results[i])
                
        return result

    def _parse_x3d_results(self, results: np.ndarray) -> Dict[str, float]:
        """Parse X3D model inference results."""
        class_names = list(self.action_classes.keys())
        
        # Get predicted class and confidence
        predicted_idx = np.argmax(results)
        predicted_action = class_names[predicted_idx] if predicted_idx < len(class_names) else "unknown"
        action_confidence = float(results[predicted_idx])
        
        # Calculate overall threat score (all non-normal classes)
        threat_indices = [i for i, name in enumerate(class_names) if name != "normal"]
        threat_scores = [results[i] for i in threat_indices if i < len(results)]
        overall_score = sum(threat_scores) if threat_scores else 0.0
        
        result = {
            "score": overall_score,
            "action": predicted_action,
            "action_confidence": action_confidence
        }
        
        # Add individual class scores
        for i, class_name in enumerate(class_names):
            if i < len(results):
                result[class_name] = float(results[i])
                
        return result

    def _parse_movinet_results(self, results: np.ndarray) -> Dict[str, float]:
        """Parse MoViNet model inference results."""
        class_names = list(self.action_classes.keys())
        
        # Get predicted class and confidence
        predicted_idx = np.argmax(results)
        predicted_action = class_names[predicted_idx] if predicted_idx < len(class_names) else "unknown"
        action_confidence = float(results[predicted_idx])
        
        # Calculate overall threat score (all non-normal classes)
        threat_indices = [i for i, name in enumerate(class_names) if name != "normal"]
        threat_scores = [results[i] for i in threat_indices if i < len(results)]
        overall_score = sum(threat_scores) if threat_scores else 0.0
        
        result = {
            "score": overall_score,
            "action": predicted_action,
            "action_confidence": action_confidence
        }
        
        # Add individual class scores
        for i, class_name in enumerate(class_names):
            if i < len(results):
                result[class_name] = float(results[i])
                
        return result
